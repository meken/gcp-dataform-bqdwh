{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "Customer Churn Analysis"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Churn prediction with Adventure Works\n",
        "\n",
        "In this notebook we'll use the staging tables created in the previous challenges and try to predict whether our customers will churn.\n",
        "\n",
        "Since the dataset doesn't contain any churn flag, we're going to start with deciding what churn means in our case and then add that information to our data so that we can train a model."
      ],
      "metadata": {
        "id": "5bchGceJvcb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters\n"
      ],
      "metadata": {
        "id": "UKKorWuguifH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CURATED_DATASET=\"curated\"  #@param {type:\"string\"}\n",
        "DWH_DATASET=\"dwh\" #@param {type:\"string\"}\n",
        "REGION=\"us-central1\"  #@param region {type:\"string\"}"
      ],
      "metadata": {
        "id": "gV1x0wdct25o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started with `pandas`\n",
        "\n",
        "The well known `pandas` framework supports reading data from bigquery tables, and Colab comes pre-installed with all of the required libraries.\n",
        "\n",
        "> BigQuery also provides the [BigFrames](https://cloud.google.com/bigquery/docs/dataframes-quickstart) package that's designed to be compatible with `pandas` data frames and can handle large amounts of data, but since we're dealing relatively small datasets we'll stick to the familiar `pandas` data frames."
      ],
      "metadata": {
        "id": "9WYdihgyeZJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_gbq(f\"${CURATED_DATASET}.stg_sales_order_header\")"
      ],
      "metadata": {
        "id": "fUsQzlgjiA_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leet's have a quick look at our data."
      ],
      "metadata": {
        "id": "vcCdOQ7HwVY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "metadata": {
        "id": "YQguJBV6iRSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Churned or not\n",
        "\n",
        "In order to decide whether a customer can be considered as _churned_ we're going to look at their last purchase date, if that's over a threshold, i.e, customer hasn't purchased anything since last _N_ days, we'll mark them as churned.\n",
        "\n",
        "> There's a number of different methods to do churn analysis, including survival analysis, time to event predictions etc. These are beyond the scope of this exercise, so we're keeping things very simple.\n",
        "\n",
        "But what's a good threshold for our dataset? Let's analyze our customer base and find out how many days have passed since the last purchase date of every customer."
      ],
      "metadata": {
        "id": "VXZvtJiqwaJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lpd = df.groupby(\"customer_id\")[\"order_date\"].max()"
      ],
      "metadata": {
        "id": "oZUA6ZlZj72O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the last purchase date for each customer, we could subtract that from the current date, but the dataset we're using (although updated for dates) only has data for a specific period. Let's find the date that we can use as the _current date_ for this dataset."
      ],
      "metadata": {
        "id": "uelihyMCvatx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mpd = max(lpd)"
      ],
      "metadata": {
        "id": "bbME6kpgll6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, we're ready to calculate the number of days since last purchase."
      ],
      "metadata": {
        "id": "I6AfLWJIxk-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "days_since_last_purchase = pd.to_datetime(mpd) - pd.to_datetime(lpd)"
      ],
      "metadata": {
        "id": "wX71qTuSl71_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does the distribution of this look like?"
      ],
      "metadata": {
        "id": "LlG55ngjxpjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "days_since_last_purchase.dt.days.hist();"
      ],
      "metadata": {
        "id": "o0sfuqnpoqEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that majority of our customers have been making relatively recent purchases, although there's a few that haven't bought anything since **3** years, those have certainly churned.\n",
        "\n",
        "That's useful information, but we need more data. We need to find out how long it takes between two consecutive purchases, to determine our potential threshold."
      ],
      "metadata": {
        "id": "q4vpoYmZxty-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diffs = df.sort_values([\"customer_id\", \"order_date\"]).groupby(\"customer_id\")[\"order_date\"].diff()"
      ],
      "metadata": {
        "id": "EGmMzN1Upxch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A picture is worth thousand words, let's visualize a histogram of this data."
      ],
      "metadata": {
        "id": "C1MOJ0obyX9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diffs.dt.days.hist();"
      ],
      "metadata": {
        "id": "2AZ-Lc5nhpxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like most purchases are done within 100 days. So, to stay on the safe side of things, we're going use **180** days (almost 6 months), to be our threshold. So, if a customer hasn't done a purchase for more than 180 days, we'll consider them as churned.  "
      ],
      "metadata": {
        "id": "10WWACBZyi4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"last_purchase_date\"] = df.groupby(\"customer_id\")[\"order_date\"].transform(\"max\")"
      ],
      "metadata": {
        "id": "394wdUHEi9oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"churned\"] = (pd.to_datetime(mpd) - pd.to_datetime(df[\"last_purchase_date\"])).dt.days > 180"
      ],
      "metadata": {
        "id": "o6IqVh5WmsYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've established our churned customer definition, let's have a look at the distribution of customers who have churned."
      ],
      "metadata": {
        "id": "EUIYU7XWy9IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"churned\")[\"churned\"].count().plot.bar();"
      ],
      "metadata": {
        "id": "X0n3PNtnm--z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That looks pretty nice and balanced, although in real world we'd expect (or hope for) less customers churning."
      ],
      "metadata": {
        "id": "TulzFSwczJvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training data\n",
        "\n",
        "Alright, we're almost ready to do some training. We've now established which customers have churned, next step is to combine that information with for example customer details, so that we can make predictions based on customer details."
      ],
      "metadata": {
        "id": "KuVdBXmi7phJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploration playground\n",
        "\n",
        "Data scientists typically need a separate place where they can create different types of derived tables, so let's create another dataset"
      ],
      "metadata": {
        "id": "dpiiQr0uu4qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! bq show exploration || bq mk --location=$REGION exploration"
      ],
      "metadata": {
        "id": "XK7KxJINu-Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a separate dataset, let's store the dataframe that we used to determine the churn information."
      ],
      "metadata": {
        "id": "YEBjPXPiy6nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tdf = df.groupby(\"customer_id\", as_index=False).max(\"churned\")"
      ],
      "metadata": {
        "id": "zhFidjSW7x6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tdf[[\"customer_id\", \"churned\"]].to_gbq(\"exploration.churn_labels\", if_exists=\"replace\")"
      ],
      "metadata": {
        "id": "coATJWD68BqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training data consists of customer details joined with the churn information, we can do that using `pandas` dataframes, or since both tables are now in BigQuery, using `SQL`."
      ],
      "metadata": {
        "id": "glO7zBvrzFO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "CREATE OR REPLACE TABLE\n",
        "  exploration.churn_training AS\n",
        "SELECT\n",
        "  c.customer_id,\n",
        "  p.*,\n",
        "  l.churned\n",
        "FROM\n",
        "  curated_test.stg_customer c,\n",
        "  curated_test.stg_person p,\n",
        "  exploration.churn_labels l\n",
        "WHERE\n",
        "  c.person_id = p.business_entity_id AND\n",
        "  c.customer_id = l.customer_id"
      ],
      "metadata": {
        "id": "dPHTX04M89Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "\n",
        "Now we have the data, we have multiple options. We can use any framework to train a new model, scikit-learn, Tensorflow, PyTorch etc. We could also use Vertex AI to do this training using a managed service on specific hardware. But since the star of this hack is BigQuery, we'll use **BQML**.\n",
        "\n",
        "> Note that we're keeping things very simple, building an end to end MLOps pipeline is beyond the scope of this hack, however if you're interested in that, we have another [gHack](https://ghacks.dev/hacks/mlops-on-gcp) specifically designed for it.\n",
        "\n",
        "Training a model with BigQuery is quite trivial, we'll stick to the defaults for most of the parameters, but see the [docs](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-glm) for more information. BigQuery even automatically [pre-processes the features](https://cloud.google.com/bigquery/docs/auto-preprocessing)!\n",
        "\n",
        "> You might have noticed that we didn't split the dataset into training and test sets, that's because BigQuery ML automatically splits the input data into training and evaluation sets, in order to avoid overfitting the model. You can however override that and provide your own split method."
      ],
      "metadata": {
        "id": "rbokY8t6esD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "CREATE OR REPLACE MODEL `$DWH_DATASET.churn_model`\n",
        "OPTIONS\n",
        "(\n",
        "  model_type='LOGISTIC_REG',\n",
        "  auto_class_weights=TRUE,\n",
        "  input_label_cols=['churned']\n",
        ") AS\n",
        "SELECT * EXCEPT(customer_id) FROM exploration.churn_training"
      ],
      "metadata": {
        "id": "HAd5wwJMi9UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training should take a few seconds as we're dealing with a small dataset that converges relatively quickly.\n",
        "\n",
        "The model is stored in BigQuery, however, it's also possible to [store it in Vertex AI Model Repository](https://cloud.google.com/bigquery/docs/create_vertex) in order to use the rest of the Vertex AI services."
      ],
      "metadata": {
        "id": "LqRjc5e1phye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Great, we have a model now, but, how good is it?"
      ],
      "metadata": {
        "id": "bxMTGRVzxPfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.EVALUATE(MODEL `$DWH_DATASET.churn_model`)"
      ],
      "metadata": {
        "id": "2N4cdcV2wzJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That doesn't look too bad for the amount of effort that we spent on this (you should see an ROC AUC value of > 0.8)!"
      ],
      "metadata": {
        "id": "7UtE3xcN0ejz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This concludes our data science adventure. With this notebook we've shown how to connect to BigQuery from an interactive environment, use familiar Python libraries and train models using BQML."
      ],
      "metadata": {
        "id": "zmWA9tJUxl9j"
      }
    }
  ]
}